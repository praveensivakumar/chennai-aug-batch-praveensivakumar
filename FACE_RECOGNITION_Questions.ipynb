{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZPrUBuA2D9n"
   },
   "source": [
    "## Deep face recognition with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "B3NdNn8BoS4y",
    "outputId": "866def55-49d8-47b3-a173-b8ae50553149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayg3Af037u5j"
   },
   "source": [
    "Set files_zip_path to the location in the drive where you have the new `'Files_required_for_face_recognition.zip'` file. This block will extract all the files to the current working directory. You should be seeing the list of all files inside the zip files as the output of this block after the final `!ls` command is executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ObkEzXr66Sz"
   },
   "outputs": [],
   "source": [
    "files_zip_path = \"/content/drive/My Drive/Residecy10/ACV_Project2/Files_required_for_face_recognition.zip\"\n",
    "\n",
    "import zipfile\n",
    "#This extracts the files to the current working directory\n",
    "archive = zipfile.ZipFile(files_zip_path, 'r')\n",
    "archive.extractall()\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jpaCy9Q1y4M"
   },
   "source": [
    "### First, lets install the required libraries. Upload the `requirements.txt` file given and run the below commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9k1hVvWLnGw"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0YcTuo3MmBS"
   },
   "outputs": [],
   "source": [
    "!pip install request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNbjzAgh3em-"
   },
   "source": [
    "### Training the network\n",
    "\n",
    "The CNN model is taken from the Keras-OpenFace project. The architecture details aren't too important here, it's only useful to know that there is a fully connected layer with 128 hidden units followed by an L2 normalization layer on top of the convolutional base. These two top layers are referred to as the embedding layer from which the 128-dimensional embedding vectors can be obtained. The complete model is defined in `model.py` and a graphical overview is given in `model.png`. A Keras version of the `nn4.small2` model can be created with `create_model()`.\n",
    "\n",
    "\n",
    "**Run the below code to initialize the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRMdAbSsMcPC"
   },
   "outputs": [],
   "source": [
    "from model import create_model\n",
    "#Disable tensorflow backend warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import warnings\n",
    "# Suppress LabelEncoder warning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nn4_small2 = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyCaIac9N8_8"
   },
   "source": [
    "#### Idea of Training the model with Triplet loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNKudbMe4u7W"
   },
   "source": [
    "Model training aims to learn an embedding f(x) of image x such that the squared L2 distance between all faces of the same identity is small and the distance between a pair of faces from different identities is large. This can be achieved with a triplet loss L that is minimized when the distance between an anchor image xai and a positive image xpi (same identity) in embedding space is smaller than the distance between that anchor image and a negative image xni (different identity) by at least a margin α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Ose4tTyPDeU"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer\n",
    "\n",
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=(96, 96, 3))\n",
    "in_p = Input(shape=(96, 96, 3))\n",
    "in_n = Input(shape=(96, 96, 3))\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "# The nn4_small model instance is shared (Siamese network)\n",
    "emb_a = nn4_small2(in_a)\n",
    "emb_p = nn4_small2(in_p)\n",
    "emb_n = nn4_small2(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "nn4_small2_train = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpTO6eimP5t0"
   },
   "outputs": [],
   "source": [
    "from data import triplet_generator\n",
    "\n",
    "# triplet_generator() creates a generator that continuously returns \n",
    "# ([a_batch, p_batch, n_batch], None) tuples where a_batch, p_batch \n",
    "# and n_batch are batches of anchor, positive and negative RGB images \n",
    "# each having a shape of (batch_size, 96, 96, 3).\n",
    "generator = triplet_generator() \n",
    "\n",
    "nn4_small2_train.compile(loss=None, optimizer='adam')\n",
    "nn4_small2_train.fit_generator(generator, epochs=10, steps_per_epoch=100)\n",
    "\n",
    "# Please note that the current implementation of the generator only generates \n",
    "# random image data. The main goal of this code snippet is to demonstrate \n",
    "# the general setup for model training. In the following, we will anyway \n",
    "# use a pre-trained model so we don't need a generator here that operates \n",
    "# on real training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AdsuNBe9OHyZ"
   },
   "source": [
    "For this project, we are considering a pre-trained model given in file path **`nn4.small2.v1.h5`**.\n",
    "\n",
    "# 1. Using **load_weights()** function load the given pre-trained weight file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oV73nKP3QCLT"
   },
   "outputs": [],
   "source": [
    "nn4_small2_train.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bulEElwQ54g6"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "re9AUK4f520z"
   },
   "source": [
    "To demonstrate face recognition on a custom dataset, a small dataset is used. It consists of around 15-25 face images of 10 different persons. The metadata for each image (file and identity name) are loaded into memory for later processing.\n",
    "\n",
    "\n",
    "Upload Images zip file given to drive and download and extract it using the below code. And we will pass the folder `images` to `load_metadata` function to save all the images filenames and person numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbDurWRMQnw7"
   },
   "source": [
    "#### Import drive module from google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KYr8qfMD7Poc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GlBCA8wQ1cX"
   },
   "source": [
    "#### Give a path to mount the files in your drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxhxrGQS7Pq2"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive/My Drive/Residecy10/ACV_Project2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CBB_OncAQ8h_"
   },
   "source": [
    "#### Using the above given mounted path, give the images.zip path dependent on where you placed the file in your drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pprDxdHT7S6N"
   },
   "outputs": [],
   "source": [
    "## For example\n",
    "images_path = \"/content/drive/My Drive/Residecy10/ACV_Project2/images.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edsF05iuRkOd"
   },
   "source": [
    "#### Using ZipFile module to extract the images zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3eIglFdVcvB"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVAjzG4IXYQX"
   },
   "outputs": [],
   "source": [
    "with ZipFile(images_path, 'r') as zip:\n",
    "  zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oesXJD9ySB6w"
   },
   "source": [
    "#### Run the below function to load the images from the extracted images folder from the above step and map each image with person id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Q7TS19vVbGb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # print(base, name, file)\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "    \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "            # Check file extension. Allow only jpg/jpeg' files.\n",
    "            ext = os.path.splitext(f)[1]\n",
    "            if ext == '.jpg' or ext == '.jpeg':\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata = load_metadata('images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDV5yBp93Y-0"
   },
   "outputs": [],
   "source": [
    "print(metadata[0].file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vr4xVNqIaHgE"
   },
   "source": [
    "### Face Crop \n",
    "The nn4.small2.v1 model was trained with cropped face images, therefore, the face images from the custom dataset must be cropped too. Here, we use the face detection model we built in Milestone 2 and use it to detect faces and crop the faces to pass to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbWhL7jbUwHg"
   },
   "source": [
    "##### 1. Import the pre-trained model from milestone 2. You would need all the python files required for the face-detection assignment and the trained weights as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FKiloh93Y_H"
   },
   "outputs": [],
   "source": [
    "from mn_model import mn_model\n",
    "from face_generator import BatchGenerator\n",
    "from keras_ssd_loss import SSDLoss\n",
    "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "img_height =512\n",
    "img_width = 512\n",
    "\n",
    "img_channels = 3\n",
    "\n",
    "n_classes =2 \n",
    "class_names = [\"background\",\"face\"]\n",
    "\n",
    "scales = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # anchorboxes for coco dataset\n",
    "aspect_ratios = [[0.5, 1.0, 2.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [1.0/3.0, 0.5, 1.0, 2.0, 3.0],\n",
    "                 [0.5, 1.0, 2.0],\n",
    "                 [0.5, 1.0, 2.0]] # The anchor box aspect ratios used in the original SSD300\n",
    "two_boxes_for_ar1 = True\n",
    "limit_boxes = True # Whether or not you want to limit the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are scaled as in the original implementation\n",
    "coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids' or 'minmax' format, see documentation\n",
    "normalize_coords = True\n",
    "\n",
    "model, model_layer, img_input, predictor_sizes = mn_model(image_size=(img_height, img_width, img_channels), \n",
    "                                                                      n_classes = n_classes,\n",
    "                                                                      min_scale = None, \n",
    "                                                                      max_scale = None, \n",
    "                                                                      scales = scales, \n",
    "                                                                      aspect_ratios_global = None, \n",
    "                                                                      aspect_ratios_per_layer = aspect_ratios, \n",
    "                                                                      two_boxes_for_ar1= two_boxes_for_ar1, \n",
    "                                                                      limit_boxes=limit_boxes, \n",
    "                                                                      variances= variances, \n",
    "                                                                      coords=coords, \n",
    "                                                                      normalize_coords=normalize_coords)\n",
    "\n",
    "#model.summary()\n",
    "model_path = './'\n",
    "model_name = 'ssd_mobilenet_face_epoch_25_loss0.0916.h5'\n",
    "\n",
    "model.load_weights(model_path + model_name,  by_name= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGNgNidM3Y_T"
   },
   "source": [
    "##### 2. Function to load an image and reverse the channels for matplot lib to show them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ape5WxvVWKOe"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0rtSevW3Y_0"
   },
   "source": [
    "You can access each image path from `metadata[i].image_path()` where, i is the image number. We can take values from 1 to no.of images in the dataset given and plot them using matplotlib's imshow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptDNq8noWK89"
   },
   "outputs": [],
   "source": [
    "# Load an image\n",
    "# for example, loading the image with index 1\n",
    "one_image = load_image(metadata[1].image_path())\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Show original image\n",
    "# OpenCV loads images with color channels\n",
    "# in BGR order. So we need to reverse them everytime we use matplotlib to show the image.\n",
    "# Otherwise you see the image in false colour\n",
    "plt.imshow(one_image)[...,::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vbIw3kKY3ZAL"
   },
   "source": [
    "##### 3. A function to get the face bounding box x_min, x_max, y_min and y_max of input image using the face detection model given an image read using cv2.imread.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILEa4E6R3ZAM"
   },
   "outputs": [],
   "source": [
    "def face_bb(test_img):\n",
    "    img_height =512\n",
    "    img_width = 512\n",
    "    img_channels = 3\n",
    "    _CONF = 0.60\n",
    "    _IOU = 0.15\n",
    "    coords = 'centroids' # Whether the box coordinates to be used as targets for the model should be in the 'centroids' or 'minmax' format, see documentation\n",
    "    normalize_coords = True\n",
    "    org_height = test_img.shape[0]\n",
    "    org_width = test_img.shape[1]\n",
    "    test_img = cv2.resize(test_img, (512, 512))\n",
    "    test_img_input = np.expand_dims(test_img,axis=0)\n",
    "    y_pred = model.predict(test_img_input)\n",
    "    y_pred_decoded = decode_y2(y_pred,\n",
    "                                 confidence_thresh=_CONF,\n",
    "                                iou_threshold=_IOU,\n",
    "                                top_k='all',\n",
    "                                input_coords=coords,\n",
    "                                normalize_coords=normalize_coords,\n",
    "                                img_height=img_height,\n",
    "                                img_width=img_width)\n",
    "    result = y_pred_decoded[0][0]\n",
    "    det_label = result[0]\n",
    "    det_conf = result[1]\n",
    "    det_xmin = result[2]\n",
    "    det_xmax = result[3]\n",
    "    det_ymin = result[4]\n",
    "    det_ymax = result[5]\n",
    "    #Converting to integers as the indexes to images are only integers\n",
    "    bb = [int(det_xmin),int(det_xmax),int(det_ymin),int(det_ymax)]\n",
    "    return bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OkENkFW3ZAZ"
   },
   "source": [
    "# 2. Write a function to get the resized cropped out face of an input image from the path given using the face_bb() function defined above. - 5 Marks\n",
    "Hint: The face_bb() function gives a list of bounding box co-ordinate values like **[x_min,x_max,y_min,y_max]**. Use **cv2.imread()** to load an image from the image path and pass it to the face_bb function as input. You can crop an image by simply accessing the specific values of the image matrix using the co-ordinates as the boundary indexes. But, before cropping we have to resize the input image to (512,512) like done in the **face_bb()** function as the bounding box co-ordinates are for the resized image. A simple crop can be implemented on the resized image by doing **\"image[y_min:y_max,x_min:x_max,:]\"** . \n",
    "You have to then resize the cropped image to a fixed size of (96,96). You can do that using the **cv2.resize()** function again. It takes the second argument as (96,96) this time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "54fWVBxn3ZAb"
   },
   "outputs": [],
   "source": [
    "import cv\n",
    "import Image\n",
    "def get_face(img_path):\n",
    "    min_size = (20,20)\n",
    "    image_scale = 1\n",
    "    haar_scale = 1.1\n",
    "    min_neighbors = 3\n",
    "    smallImage = cv.CreateImage((cv.Round(image.width / image_scale),cv.Round(image.height / image_scale)), 8 ,1)\n",
    "    # Scale input image for faster processing\n",
    "    cv.Resize(image, smallImage, cv.CV_INTER_LINEAR)\n",
    "    cv.EqualizeHist(smallImage, smallImage)\n",
    "\n",
    "    # Detect the faces\n",
    "    faces = cv.HaarDetectObjects(\n",
    "            smallImage, faceCascade, cv.CreateMemStorage(0),\n",
    "            haar_scale, min_neighbors, haar_flags, min_size\n",
    "        )\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpe2G6JSbTis"
   },
   "source": [
    "# 3. Write code to load 2nd and 3rd images in the metadata using load_image() and show each image and its cropped version side by side for comparison using matplotlib imshow - 7.5 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LkBQRL_sd2U8"
   },
   "source": [
    "### Generate embeddings for each image in the dataset\n",
    "\n",
    "Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S01r8UzXc-8s"
   },
   "source": [
    "#### Get embedding vector for first image in the metadata using the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2yd69OydBAq"
   },
   "outputs": [],
   "source": [
    "# Align the image\n",
    "cropped_img = get_face(metadata[0].image_path())\n",
    "\n",
    "# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\n",
    "img = (cropped_img / 255.).astype(np.float32)\n",
    "\n",
    "# obtain embedding vector for an image\n",
    "embedding_vector = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\n",
    "\n",
    "print(embedding_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "plHvUTytcTGo"
   },
   "source": [
    "# 4. Write code to iterate through metadata and create embeddings for each image using nn4_small2_pretrained.predict() and store in a list with name `embeddings` - 5 marks\n",
    "\n",
    "If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 128-zeroes as the final embedding from the model is of length 128.\n",
    "Hint: Don't forget to use numpy's expand_dims funtion to convert each image into the shape (1,96,96,3) and take the 0th element of the prediction from the model as the embedding of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yY9ykxtueY4k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hb3XSDsfTMG"
   },
   "source": [
    "#### Write code to get the distance between the respective embeddings given 2 pairs of images.\n",
    "\n",
    "Consider distance metric as \"Squared L2 distance\"\n",
    "\n",
    "squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2\n",
    "\n",
    "\n",
    "\n",
    "# 5. Plot images and get distance between the pairs given below. - 5 Marks\n",
    "\n",
    "1. 10,12 and 10,131\n",
    "\n",
    "2. 30,31 and 30,100\n",
    "\n",
    "3. 70,72 and 70,115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDVLED10eboB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_HfgsQXgV6u"
   },
   "source": [
    "#### Now lets build a simple fully connected neural network classifier to predict person in the given image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AahHoLK1hZJj"
   },
   "source": [
    "If you observe distances between more pairs the difference is not constant between opposite pairs. So we train a fully connected neural network with very limited dataset to classiffy each embedding into the person. Lets prepare the data for training a fully connected neural network using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gfLzoBIhrpV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "targets = np.array([int(m.name) for m in metadata])\n",
    "\n",
    "num_classes = 11\n",
    "# convert class vectors to one-hot encodings\n",
    "y = keras.utils.to_categorical(targets, num_classes)\n",
    "\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "## checking the shapes of metaadata and test and train sets\n",
    "print(metadata.shape)\n",
    "print(train_idx.shape)\n",
    "print(test_idx.shape)\n",
    "\n",
    "\n",
    "# one half as train examples of 10 identities\n",
    "X_train = embeddings[train_idx]\n",
    "X_test = embeddings[test_idx]\n",
    "\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "# svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "312NcUeuiNpL"
   },
   "source": [
    "# 6. Build a 1 layered fully connected neural network using keras and report the accuracy - 7.5 marks\n",
    "Hint: Create only one layer with 11 neurons as we have 11 classes and mention the input_dim as 128 as the embeddings are of 128 length. Use a small batch size like 4 and use softmax as the activation function of the layer. \n",
    "You might have to train upto 80 epochs to get decent accuracy. You can use an Adam optimizer for the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ybglrDph7tJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqFfYIZbiXG8"
   },
   "source": [
    "# 7.  Test the classifier \n",
    "\n",
    "Take 35th image from test set and plot the image, report to which person(folder name in dataset) the image belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2E6WBuMiBmX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmfxVcBH3ZBp"
   },
   "source": [
    "## Optional Exercise - For people who are familiar with SVMs, you can use an SVM classifier(If you already know how to use the scikit-learn package for it) to do this last part being done by the fully connected neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xCUexkj3ZBq"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "targets = np.array([m.name for m in metadata])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "## checking the shapes of metaadata and test and train sets\n",
    "print(metadata.shape)\n",
    "print(train_idx.shape)\n",
    "print(test_idx.shape)\n",
    "\n",
    "\n",
    "# one half as train examples of 10 identities\n",
    "X_train = embeddings[train_idx]\n",
    "# another half as test examples of 10 identities\n",
    "X_test = embeddings[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "svc = LinearSVC()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(f'SVM accuracy = {acc_svc}')\n",
    "\n",
    "import warnings\n",
    "# Suppress LabelEncoder warning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "example_idx = 44\n",
    "\n",
    "example_image = load_image(metadata[test_idx][example_idx].image_path())\n",
    "example_prediction = svc.predict([embeddings[test_idx][example_idx]])\n",
    "example_identity = encoder.inverse_transform(example_prediction)[0]\n",
    "\n",
    "plt.imshow(example_image)\n",
    "plt.title(f'Identified as {example_identity}');"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FACE_RECOGNITION_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
