{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Rc_ahEnTF9m"
   },
   "source": [
    "# Predict tags on StackOverflow with linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5hmogPBTF9p"
   },
   "source": [
    "In this assignment you will learn how to predict tags for posts from [StackOverflow](https://stackoverflow.com). To solve this task you will use multilabel classification approach.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "In this task you will need the following libraries:\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [Pandas](https://pandas.pydata.org) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [NLTK](http://www.nltk.org) — a platform to work with natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zSq4Uzh6TF9q"
   },
   "source": [
    "### Data\n",
    "\n",
    "You can find all data required for this assignment into the folder `/data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2V1gba1KTF9r"
   },
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnvsWcP9TF9t"
   },
   "source": [
    "For this assignment you will need to use a list of stop words. It can be downloaded from *nltk*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "R-RxVRvsTF9u",
    "outputId": "c1805543-4ac0-4486-e19c-b70cc1efa7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ErHWjimITF91"
   },
   "source": [
    "In this task you will deal with a dataset of post titles from StackOverflow. You are provided a split to 3 sets: *train*, *validation* and *test*. All corpora (except for *test*) contain titles of the posts and corresponding tags (100 tags are available). The *test* set doesn't contain answers. Upload the corpora using *pandas* and look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "-ufeO-nfVGha",
    "outputId": "f42e59bd-2e3c-4718-ef47-1b2245ead5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Importing data from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NoYCYlXRXu4j"
   },
   "outputs": [],
   "source": [
    "project_path = './drive/My Drive/ColabNotebooks/NLP/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDahiblaU-e7"
   },
   "source": [
    "Literal_eval package takes care of the preprocessing for the string so that it can be used in python. To know more on literal_eval please see the below documentation <br>\n",
    "https://kite.com/python/docs/ast.literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bubX7TSLVLXR"
   },
   "source": [
    "## Task 1: Create training, testing and validation data from the files given. Use title to be the independent variable and tags to be the dependent variable ( 5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBFMf3bhVPb_"
   },
   "source": [
    "Note: Ensure you apply literal_eval function on the tags column to ensure all the tags are readable in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x8iH7o77TF96"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(project_path+'train.tsv',sep='\\t')\n",
    "test_data = pd.read_csv(project_path+'test.tsv',sep='\\t')\n",
    "val_data = pd.read_csv(project_path+'validation.tsv',sep='\\t')\n",
    "train_data.tags = train_data.tags.apply(literal_eval)\n",
    "val_data.tags = val_data.tags.apply(literal_eval)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4pPOmFvTF-B"
   },
   "outputs": [],
   "source": [
    "x_train = train_data.title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhtWz8ymV3x9"
   },
   "outputs": [],
   "source": [
    "x_val = val_data.title.values\n",
    "x_test = test_data.title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USlubitlTF-G"
   },
   "outputs": [],
   "source": [
    "y_train = train_data.tags.values\n",
    "y_val = val_data.tags.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwhOVHwjZOrr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bcb3kXUSTF-M"
   },
   "source": [
    "## Task 2 (Pre-processing). Implement the function *text_prepare* following the instructions. After that, run the function *test_test_prepare* to test it on tiny cases. (10 points)\n",
    "\n",
    "One of the most known difficulties when working with natural data is that it's unstructured. For example, if you use it \"as is\" and extract tokens just by splitting the titles by whitespaces, you will see that there are many \"weird\" tokens like *3.5*, *?*,  *{}*, etc. To prevent the problems, it's usually useful to prepare the data in a custom way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBSchmxtTF-N"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iI1plv6WTF-R"
   },
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def text_prepare(text): ### The function will take in text and lower case it remove the stopwords, symbols and return it.\n",
    "    text = text.lower()             ### Write a code which can change the input text to lowercase.\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)             ### Write a code which replaces REPLACE_BY_SPACE_RE (above mentioned) symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)             ### Write a code which deletes symbols which are in BAD_SYMBOLS_RE (above mentioned) from text\n",
    "    text = remove_stopwords(text, STOPWORDS)            ### Write a code which deletes stopwords from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5LQEgMATF-Y"
   },
   "outputs": [],
   "source": [
    "def test_text_prepare():\n",
    "    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n",
    "                \"How to free c++ memory vector<int> * arr?\"]\n",
    "    answers = [\"sql server equivalent excels choose function\", \n",
    "               \"free c++ memory vectorint arr\"]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if text_prepare(ex) != ans:\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkxNVLPpeGPW"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "def remove_stopwords(text, STOPWORDS):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6t2DpralVokG"
   },
   "source": [
    "Execute the test_text_prepare function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ihi8I2yQTF-b",
    "outputId": "e4b55ca6-9a4c-4a10-fc32-7ce2b3aa9f3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basic tests are passed.'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ettHAZ8eVt1R"
   },
   "source": [
    "*Note: You should pass the above test to ensure the text preprocessing is done before our analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QV5JC6HXTF-k"
   },
   "source": [
    "Now we can preprocess the titles using function *text_prepare* and  making sure that the headers don't have bad symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOZKuHnGTF-k"
   },
   "outputs": [],
   "source": [
    "x_train = [text_prepare(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "25COG-1-WBeh"
   },
   "outputs": [],
   "source": [
    "x_val = [text_prepare(x) for x in x_val]\n",
    "x_test =[text_prepare(x) for x in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfWls32MV5Uu"
   },
   "source": [
    "Print the top 5 elements in x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "3BP3LiGsTF-o",
    "outputId": "8a587142-8ebc-4602-b088-75097395df59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['draw stacked dotplot r',\n",
       " 'mysql select records datetime field less specified value',\n",
       " 'terminate windows phone 81 app',\n",
       " 'get current time specific country via jquery',\n",
       " 'configuring tomcat use ssl']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "At_ZTeqTTF-t"
   },
   "source": [
    "## Task 2 (WordsTagsCount) - Find 3 most popular tags and 3 most popular words in the train data. - 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2mT1f7gWKSu"
   },
   "source": [
    "Note: The words which appear the most are considered as popular in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-eQoxipTF-v"
   },
   "outputs": [],
   "source": [
    "tagsDic = {}\n",
    "wordsDic = {}\n",
    "wordLst = ''.join(x_train).split()\n",
    "from collections import Counter\n",
    "counter = Counter(wordLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZsmC0y-s8Mx"
   },
   "outputs": [],
   "source": [
    "wordsDic = {token:count for token,count in counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgPE_DYGtKbu"
   },
   "outputs": [],
   "source": [
    "tagLst = []\n",
    "for tags in y_train:\n",
    "  for tag in tags:\n",
    "    tagLst.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45K4ZhrKe-u1"
   },
   "outputs": [],
   "source": [
    "couter2 = Counter(tagLst)\n",
    "tagsDic = {token:count for token,count in couter2.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xkwg8_WNTF-3"
   },
   "source": [
    "We are assuming that *tags_counts* and *words_counts* are dictionaries like `{'some_word_or_tag': frequency}`. After applying the sorting procedure, results will be look like this: `[('most_popular_word_or_tag', frequency), ('less_popular_word_or_tag', frequency), ...]`.\n",
    "\n",
    "eg: \n",
    "Tag 1 - 100 Tag 2 - 65 Tag 3 - 250 <br>\n",
    "after sorting looks like, <br>\n",
    "Tag 3 - 250 Tag 1 - 100 Tag 2 - 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSCPa54uTF-4"
   },
   "outputs": [],
   "source": [
    "sorted_Words = sorted(wordsDic.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAGkf78zf7Lf"
   },
   "outputs": [],
   "source": [
    "sorted_Tags = sorted(tagsDic.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ScZZqYxggnCZ",
    "outputId": "49f703a5-2630-4a33-aae7-6cb3bcb79312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('javascript', 19078), ('c#', 19077), ('java', 18661)]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 3 tags\n",
    "sorted_Tags[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "721104oJg3HD",
    "outputId": "d6215ab2-449a-4757-999e-477db5b419b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('using', 7047), ('file', 3847), ('data', 2823)]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 3 Words\n",
    "sorted_Words[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xk7M4uJ_TF-8"
   },
   "source": [
    "## Task - 3 Transforming text to a vector (10 points)\n",
    "\n",
    "Machine Learning algorithms work with numeric data and we cannot use the provided text data \"as is\". There are many ways to transform text data to numeric vectors. In this task you will try to use two of them.\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "One of the well-known approaches is a *bag-of-words* representation. To create this transformation, follow the steps:\n",
    "1. Find *N* most popular words in train corpus and numerate them. Now we have a dictionary of the most popular words.\n",
    "2. For each title in the corpora create a zero vector with the dimension equals to *N*.\n",
    "3. For each text in the corpora iterate over words which are in the dictionary and increase by 1 the corresponding coordinate.\n",
    "\n",
    "Let's try to do it for a toy example. Imagine that we have *N* = 4 and the list of the most popular words is \n",
    "\n",
    "    ['hi', 'you', 'me', 'are']\n",
    "\n",
    "Then we need to numerate them, for example, like this: \n",
    "\n",
    "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "\n",
    "And we have the text, which we want to transform to the vector:\n",
    "\n",
    "    'hi how are you'\n",
    "\n",
    "For this text we create a corresponding zero vector \n",
    "\n",
    "    [0, 0, 0, 0]\n",
    "    \n",
    "And iterate over all words, and if the word is in the dictionary, we increase the value of the corresponding position in the vector:\n",
    "\n",
    "    'hi':  [1, 0, 0, 0]\n",
    "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
    "    'are': [1, 0, 0, 1]\n",
    "    'you': [1, 1, 0, 1]\n",
    "\n",
    "The resulting vector will be \n",
    "\n",
    "    [1, 1, 0, 1]\n",
    "   \n",
    "Implement the described encoding in the function *my_bag_of_words* with the size of the dictionary equals to 5000. To find the most common words use train data. You can test your code using the function *test_my_bag_of_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KljxJWe1Vnzk"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ER1oq48TF-9"
   },
   "outputs": [],
   "source": [
    "DICT_SIZE = 5000\n",
    "commonWords = sorted_Words[:DICT_SIZE]\n",
    "INDEX_TO_WORDS = {index:word for index, word in enumerate(sorted(word for word,count in commonWords))}\n",
    "WORDS_TO_INDEX = {word:index for index, word in enumerate(sorted(word for word,count in commonWords))}\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()\n",
    "\n",
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    text_list = text.split()\n",
    "    for word in text_list:\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]] = 1\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnLYWnsaTF_A"
   },
   "outputs": [],
   "source": [
    "def test_my_bag_of_words():\n",
    "    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "    examples = ['hi how are you']\n",
    "    answers = [[1, 1, 0, 1]]\n",
    "    for ex, ans in zip(examples, answers):\n",
    "        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n",
    "            return \"Wrong answer for the case: '%s'\" % ex\n",
    "    return 'Basic tests are passed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cqWhoF24TF_D",
    "outputId": "9f6afd19-b4e8-4206-8494-328299943e3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basic tests are passed.'"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_my_bag_of_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXW3ALCITF_H"
   },
   "source": [
    "Now apply the implemented function to all samples (this might take up to a minute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m39xHB2yTF_H"
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "6DZwPWB1TF_K",
    "outputId": "408eb3bd-6f24-42be-c3b6-d48699a6adf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (100000, 5000)\n",
      "X_val shape  (30000, 5000)\n",
      "X_test shape  (20000, 5000)\n"
     ]
    }
   ],
   "source": [
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in x_train])\n",
    "X_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in x_val])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in x_test])\n",
    "print('X_train shape ', X_train_mybag.shape)\n",
    "print('X_val shape ', X_val_mybag.shape)\n",
    "print('X_test shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPXTUeNSTF_N"
   },
   "source": [
    "As you might notice, we transform the data to sparse representation, to store the useful information efficiently. There are many types: of such representations, however sklearn algorithms can work only with  csr matrix, so we will use this one.<br>\n",
    "<u>Documentations on sparse matrix:</u> <br>\n",
    "(https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) \n",
    "(https://docs.scipy.org/doc/scipy/reference/sparse.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8AMzsVxTF_Q"
   },
   "source": [
    "\n",
    "For the 11th row in *X_train_mybag* find how many non-zero elements it has. In this task the answer (variable *non_zero_elements_count*) should be a number, e.g. 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bOetzSLzTF_S",
    "outputId": "c87335ce-8159-4cc5-aea7-2b45beb40e79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_mybag[10].toarray()[0] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GYAFKGfKTF_V"
   },
   "source": [
    "## Task 4 - TF-IDF (5 points)\n",
    "\n",
    "The second approach extends the bag-of-words framework by taking into account total frequencies of words in the corpora. It helps to penalize too frequent words and provide better features space. \n",
    "\n",
    "Implement function *tfidf_features* using class  from *scikit-learn*. Use *train* corpus to train a vectorizer. Don't forget to take a look into the arguments that you can pass to it. We suggest that you filter out too rare words (occur less than in 5 titles) and too frequent words (occur more than in 90% of the titles). Also, use bigrams along with unigrams in your vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1s3uEKmjYpuh"
   },
   "source": [
    "## Write a function which takes x_train, x_val and x_test as input and return the tf-idf features of the same and the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkz1RYsBTF_Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6V7FPirTF_i"
   },
   "outputs": [],
   "source": [
    "def tfidf_features(X_train, X_val, X_test):\n",
    "  tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),token_pattern='(\\S+)')\n",
    "  X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "  X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "  X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "  return X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySnV2zCBTF_p"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(x_train, x_val, x_test)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_aopTTNL5G1"
   },
   "source": [
    "Print the index of string \"C#\" in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kZ-3wqTTMCRi",
    "outputId": "49497e55-1d30-456d-c8db-4fe6659adc6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33589"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vocab['c#']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYYO2kdiTF_6"
   },
   "source": [
    "## Task 5: Classification (15 points)\n",
    "MultiLabel classifier\n",
    "\n",
    "As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose it is convenient to use MultiLabelBinarizer from sklearn. <br>\n",
    "<u>Documentation:</u> <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ojeCe_e_TF_7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8JYQr8YTF_-"
   },
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=sorted(tagsDic.keys()))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_val = mlb.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qN1NUA3DTGAB"
   },
   "source": [
    "In this task we suggest to use One-vs-Rest approach, which is implemented in [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) class. In this approach *k* classifiers (= number of tags) are trained. As a basic classifier, use [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time, because a number of classifiers to train is large.\n",
    "\n",
    "**OneVsRest multi-label strategy**\n",
    "\n",
    "The Multi-label algorithm accepts a binary mask over multiple labels. The result for each prediction will be an array of 0s and 1s marking which class labels apply to each row input sample.\n",
    "\n",
    "**Logistic Regression & SVM**\n",
    "\n",
    "OneVsRest strategy can be used for multi-label learning, where a classifier is used to predict multiple labels for instance. LR & SVM supports multi-class, but we are in a multi-label scenario, therefore, we wrap classifiers in the OneVsRestClassifier.\n",
    "\n",
    "*If you want to learn more about OneVsRest, check out these links:*\n",
    "- *https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5*\n",
    "- *https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff*\n",
    "- *https://medium.com/coinmonks/multi-label-classification-blog-tags-prediction-using-nlp-b0b5ee6686fc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Pj6MDkbTGAC"
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUHZR2TWTGAF"
   },
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    # Create and fit LogisticRegression and LinearSVC wraped into OneVsRestClassifier.\n",
    "\n",
    "    clasf = OneVsRestClassifier(LogisticRegression())\n",
    "    clasf.fit(X_train, y_train)\n",
    "    \n",
    "    return clasf  ### clf is the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6Mkn0WRiv-f"
   },
   "outputs": [],
   "source": [
    "def train_classifier_svc(X_train, y_train):\n",
    "    # Create and fit LogisticRegression and LinearSVC wraped into OneVsRestClassifier.\n",
    "\n",
    "    clasf = OneVsRestClassifier(LinearSVC())\n",
    "    clasf.fit(X_train, y_train)\n",
    "    \n",
    "    return clasf  ### clf is the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k660NfRFTGAL"
   },
   "source": [
    "Train the classifiers for different data transformations: *bag-of-words* and *tf-idf*.\n",
    "classifier_mybag = model for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXyhN-yDTGAP"
   },
   "outputs": [],
   "source": [
    "classifier_mybow = train_classifier_svc(X_train_mybag, y_train)\n",
    "classifier_tfidf = train_classifier_svc(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ijQZCd9cTGAU"
   },
   "source": [
    "Now you can create predictions for the data. You will need two types of predictions: labels and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0QT7uWddTGAV"
   },
   "outputs": [],
   "source": [
    "y_val_predicted_labels_mybow = classifier_mybow.predict(X_val_mybag)\n",
    "y_val_predicted_scores_mybow = classifier_mybow.decision_function(X_val_mybag)\n",
    "\n",
    "y_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n",
    "y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9n0TEtHTGAj"
   },
   "source": [
    "Now, we would need to compare the results of different predictions, e.g. to see whether TF-IDF transformation helps or to try different regularization techniques in logistic regression. For all these experiments, we need to setup evaluation procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVqAxDxqTGAk"
   },
   "source": [
    "## Evaluation (10 points)\n",
    "\n",
    "To evaluate the results we will use several classification metrics:\n",
    " - [Accuracy](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    " - [F1-score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    " - [Area under ROC-curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
    " - [Area under precision-recall curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) \n",
    " \n",
    "Make sure you are familiar with all of them. If you want a refresher, you can click the link to their documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YmNhVsU9K9-"
   },
   "source": [
    "## Import the necessary libraries for the above metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBvrkOUYTGAl"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNkYBl_wTGAp"
   },
   "source": [
    "Define the function *print_evaluation_scores* which takes y_val and predicted as input calculates and prints the following output:\n",
    " - *accuracy*\n",
    " - *F1-score - Average = 'weighted'* \n",
    " - *Precision - Average = 'macro'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4nevWnhTGAq"
   },
   "outputs": [],
   "source": [
    "def print_evaluation_scores(y_val, predicted):\n",
    "    print(accuracy_score(y_val, predicted))\n",
    "    print(f1_score(y_val, predicted, average='weighted'))\n",
    "    print(precision_score(y_val, predicted,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "colab_type": "code",
    "id": "G92GIO4dTGAt",
    "outputId": "709a8798-1f7d-4585-8d22-40940cc29ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "0.3631\n",
      "0.6556284206009145\n",
      "0.6853330488362107\n",
      "Tfidf\n",
      "0.38426666666666665\n",
      "0.6801389254618738\n",
      "0.7081072067666235\n"
     ]
    }
   ],
   "source": [
    "print('THE Bag-of-words SCORE')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_mybow)\n",
    "print('THE Tfidf SCORE')\n",
    "print_evaluation_scores(y_val, y_val_predicted_labels_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kT7oso8ZTGAv"
   },
   "source": [
    "You might also want to plot some form of the [ROC curve](http://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc) for the case of multi-label classification. The input parameters for the roc curve are:\n",
    " - true labels\n",
    " - decision functions scores\n",
    " - number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4hbW3SeBVxz"
   },
   "source": [
    "Import the roc_auc function from the metrics.py file provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yofS_v3Pl72v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "G_9M0AI4TGAv",
    "outputId": "3484f101-b02c-422b-c165-2b838e8429c8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "sZaymN4uTGA0",
    "outputId": "44d29325-0d60-484c-e259-4371a63b9676"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwNpUEVLTGA2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9S3Ddz7LTGA3"
   },
   "source": [
    "## Task 4 (MultilabelClassification) - Optional \n",
    "** Once we have the evaluation set up, we suggest that you experiment a bit with training your classifiers. We will use *F1-score weighted* as an evaluation metric. Our recommendation:\n",
    "- compare the quality of the bag-of-words and TF-IDF approaches and chose one of them.\n",
    "- for the chosen one, try *L1* and *L2*-regularization techniques in Logistic Regression with different coefficients (e.g. C equal to 0.1, 1, 10, 100).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ers40KuOTGA4"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdqTO269TGA8"
   },
   "source": [
    "When you are happy with the quality, create predictions for *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M51F10f9TGBB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Stats_NLP_Project_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
