{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFfDTfhlaEI_"
   },
   "source": [
    "# Transfer Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNwbqCFRaEJC"
   },
   "source": [
    "* Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n",
    "* Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUB1uDW_8XIy"
   },
   "source": [
    "## 1. Import necessary libraries for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1546,
     "status": "ok",
     "timestamp": 1552822409084,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "Rsj4t5HTaEJE",
    "outputId": "a7dd0ec0-f64e-4f07-bc65-0ca61e49641e"
   },
   "outputs": [],
   "source": [
    "#Importing important modules\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1526,
     "status": "ok",
     "timestamp": 1552822409087,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "KpbkRBH45Rcf",
    "outputId": "33527b3a-783a-4473-e67e-a75fcaafd4d3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXrn3heBaEJa"
   },
   "source": [
    "## 2. Import MNIST data and create 2 datasets with one dataset having digits from 0 to 4 and other from 5 to 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjDuiK6ztgOK"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load/Prep the Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "split_condition1 = y_train <5\n",
    "split_condition2 = y_train >=5\n",
    "# labels = y_train[[y_train[5]]\n",
    "x_train_Ind04 = x_train [split_condition1]\n",
    "x_test_Ind04 = x_test [y_test<5]\n",
    "y_train_Ind04 = y_train[split_condition1]\n",
    "y_test_Ind04 = y_test[y_test<5]\n",
    "\n",
    "x_train_Ind59 = x_train [split_condition2]\n",
    "x_test_Ind59 = x_test [y_test>=5]\n",
    "y_train_Ind59 = y_train[split_condition2]\n",
    "y_test_Ind59 = y_test[y_test>=5]\n",
    "#mnist_df = mnist.load_data()\n",
    "#split1, split2 = tf.split(value, num_or_size_splits=2, axis=1)\n",
    "# df_1 = \n",
    "# df_2 = \n",
    "# x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') #\n",
    "# x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')\n",
    "# x_train /= 255 # normalization since we have max of 255\n",
    "# x_test /= 255\n",
    "# y_train = np_utils.to_categorical(y_train_num, 10) # since it is a classification problem we are explicitly mentioning this\n",
    "# y_test = np_utils.to_categorical(y_test_num, 10)\n",
    "\n",
    "# print('--- THE DATA ---')\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1937,
     "status": "ok",
     "timestamp": 1552822409532,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "b64-b1rl70AR",
    "outputId": "688b69b8-6b9e-4807-bbe1-e3183e1b6232"
   },
   "outputs": [],
   "source": [
    "#y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qU14lYL9A5g"
   },
   "source": [
    "## 3. Print x_train, y_train, x_test and y_test for both the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3654
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1924,
     "status": "ok",
     "timestamp": 1552822409533,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "Z9OrszhJ0SgJ",
    "outputId": "ea48d5dd-4e6b-4b34-dd54-1c9f890e8830"
   },
   "outputs": [],
   "source": [
    "print('x_train after 1st half:',x_train_Ind04)\n",
    "print('y_train after 1st half',y_train_Ind04)\n",
    "print('x_train after 2nd half',x_train_Ind59)\n",
    "print('x_train after 2nd half',y_train_Ind59)\n",
    "\n",
    "print('x_test after 1st half',x_test_Ind04)\n",
    "print('y_test after 1st half ',y_test_Ind04)\n",
    "print('x_test after 2nd half ',x_test_Ind59)\n",
    "print('x_test after 2nd half ',y_test_Ind59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJswV4xk9jQS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cB9BPFzr9oDF"
   },
   "source": [
    "## ** 4. Let us take only the dataset (x_train, y_train, x_test, y_test) for Integers 0 to 4 in MNIST **\n",
    "## Reshape x_train and x_test to a 4 Dimensional array (channel = 1) to pass it into a Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1910,
     "status": "ok",
     "timestamp": 1552822409539,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "FlQRPfFzaEJx",
    "outputId": "229430bb-7410-4767-aea9-31c889be8a80"
   },
   "outputs": [],
   "source": [
    "print(x_train_Ind04.shape)\n",
    "print(y_train_Ind04.shape)\n",
    "print(x_test_Ind04.shape)\n",
    "print(y_test_Ind04.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1898,
     "status": "ok",
     "timestamp": 1552822409541,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "y1SDuEJCHU6m",
    "outputId": "e8608078-d85d-4c46-8dc4-de9a28bad9c1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train_Ind04 = np.reshape(x_train_Ind04, (30596, 28, 28,1))\n",
    "print(x_train_Ind04.shape)\n",
    "x_test_Ind04 = np.reshape(x_test_Ind04, (5139, 28, 28,1))\n",
    "print(x_test_Ind04.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLQr-b3F-hw8"
   },
   "source": [
    "## 5. Normalize x_train and x_test by dividing it by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2256,
     "status": "ok",
     "timestamp": 1552822409908,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "PlEZIAG5-g2I",
    "outputId": "9e5cd2db-3db5-4058-da13-22b87cd2ff0a"
   },
   "outputs": [],
   "source": [
    "x_train_Ind04_Normalized = x_train_Ind04/255\n",
    "print(x_train_Ind04_Normalized)\n",
    "x_test_Ind04_Normalized = x_test_Ind04/255\n",
    "print(x_test_Ind04_Normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pytVBaw4-vMi"
   },
   "source": [
    "## 6. Use One-hot encoding to divide y_train and y_test into required no of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2345
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2237,
     "status": "ok",
     "timestamp": 1552822409909,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "V48xiua4-uUi",
    "outputId": "2db1670e-ebcb-4c7f-9ead-417a888e703b"
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices using one hot encoding\n",
    "# y_train_Ind04_Onehot = keras.utils.to_categorical(y_train_Ind04)\n",
    "# print(y_train_Ind04_Onehot)\n",
    "# y_test_Ind04_Onehot = keras.utils.to_categorical(y_test_Ind04)\n",
    "# print(y_test_Ind04_Onehot)\n",
    "\n",
    "# We use get dummies because when to categorical is used for one hot encoding, it takes from 0 to 4 and 5 to 9 instead of all the way from 0 to 9  even after splitting into 2 halves.\n",
    "# so we go for get dummies and not to categorical\n",
    "import pandas as pd\n",
    "y_train_Ind04_Onehot = pd.get_dummies(y_train_Ind04)\n",
    "print(y_train_Ind04_Onehot)\n",
    "y_test_Ind04_Onehot = pd.get_dummies(y_test_Ind04)\n",
    "print(y_test_Ind04_Onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elPkI44g_C2b"
   },
   "source": [
    "## 7. Build a sequential model with 2 Convolutional layers with 32 kernels of size (3,3) followed by a Max pooling layer of size (2,2) followed by a drop out layer to be trained for classification of digits 0-4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oR8hhRmHaM-a"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10, mnist\n",
    "from keras.models import Sequential# sequential is one type of model; there are graph models as well\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape # Dense is fully connected layer\n",
    "from keras.layers import Convolution2D, MaxPooling2D #\n",
    "from keras.utils import np_utils\n",
    "import pickle # serialization fancy word for storing on disk\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91560,
     "status": "ok",
     "timestamp": 1552822586847,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "MU09mm9F89gO",
    "outputId": "780c8135-3c5a-481a-98c8-e51cba7e264c"
   },
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10 \n",
    "\n",
    "# Define the Type of Model\n",
    "model2 = Sequential()\n",
    "# 1st Conv Layer\n",
    "model2.add(Convolution2D(32, 3, 3, input_shape=(28, 28,1)))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# 2nd Conv Layer\n",
    "model2.add(Convolution2D(32, 3, 3))# input shape is known in the previous layer. so we don't give input shape\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "# Fully Connected Layer\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128))\n",
    "model2.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# Layer 1\n",
    "model2.add(Dense(output_dim=5, bias=True))\n",
    "model2.add(Activation(\"softmax\"))\n",
    "\n",
    "# Loss and Optimizer\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Store Training Results\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "callback_list = [early_stopping]# [stats, early_stopping]\n",
    "\n",
    "# Train the model\n",
    "model2.fit(x_train_Ind04_Normalized, y_train_Ind04_Onehot, nb_epoch=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(x_test_Ind04_Normalized, y_test_Ind04_Onehot), callbacks=callback_list, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJQaycRO_3Au"
   },
   "source": [
    "## 8. Post that flatten the data and add 2 Dense layers with 128 neurons and neurons = output classes with activation = 'relu' and 'softmax' respectively. Add dropout layer inbetween if necessary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOZeRbK7t9AT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "my1P09bxAv8H"
   },
   "source": [
    "## 9. Print the training and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3942,
     "status": "ok",
     "timestamp": 1552823052242,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "yf7F8Gdutbf0",
    "outputId": "f62472f7-bd96-4945-e6e4-fa11a04cc706"
   },
   "outputs": [],
   "source": [
    "model2.evaluate(x_train_Ind04, y_train_Ind04_Onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1594,
     "status": "ok",
     "timestamp": 1552823207172,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "_N7eUBiHcRbX",
    "outputId": "7a1055d9-9cf4-4b02-f72d-293b128b6e14"
   },
   "outputs": [],
   "source": [
    "model2.evaluate(x_test_Ind04, y_test_Ind04_Onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z78o3WIjaEJ3"
   },
   "source": [
    "## 10. Make only the dense layers to be trainable and convolutional layers to be non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1004,
     "status": "ok",
     "timestamp": 1552823003197,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "brN7VZHFaEJ4",
    "outputId": "951dafb0-90c1-45da-dfd5-edba2c1defb8"
   },
   "outputs": [],
   "source": [
    "#Freezing layers in the model which don't have 'dense' in their name\n",
    "for layer in model2.layers:\n",
    "  if('dense' not in layer.name): #prefix detection to freeze layers which does not have dense\n",
    "    #Freezing a layer\n",
    "    layer.trainable = False # freezing the layer\n",
    "\n",
    "#Module to print colourful statements\n",
    "from termcolor import colored\n",
    "\n",
    "#Check which layers have been frozen \n",
    "for layer in model2.layers:\n",
    "  print (colored(layer.name, 'blue'))\n",
    "  print (colored(layer.trainable, 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4opnW7o0BJ8P"
   },
   "source": [
    "## 11. Use the model trained on 0 to 4 digit classification and train it on the dataset which has digits 5 to 9  (Using Transfer learning keeping only the dense layers to be trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1552824900206,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "lCFcYHTm6-cE",
    "outputId": "513831a1-834b-42c2-ae2c-f78fc00d5e69"
   },
   "outputs": [],
   "source": [
    "# repeat all the pre-processing steps for 5 to 9 like in 0 to 4 and then use the 0 to 4 model's learning via transfer learning to 5 to 9 data set\n",
    "print(x_train_Ind59.shape)\n",
    "print(y_train_Ind59.shape)\n",
    "print(x_test_Ind59.shape)\n",
    "print(y_test_Ind59.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1113,
     "status": "ok",
     "timestamp": 1552824903354,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "YMl9Vro0fjOO",
    "outputId": "8c224e7a-e1d7-4f75-e885-4892ac12e709"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train_Ind59 = np.reshape(x_train_Ind59,(x_train_Ind59.shape[0], 28, 28,1))\n",
    "print(x_train_Ind59.shape)\n",
    "x_test_Ind59 = np.reshape(x_test_Ind59,(x_test_Ind59.shape[0], 28, 28,1))\n",
    "print(x_test_Ind59.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 11181
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1552824909072,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "iNxTHmK3fjSa",
    "outputId": "bb8eb773-e602-4cf8-b3b8-96817026af66"
   },
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "x_train_Ind59_Normalized = x_train_Ind59/255\n",
    "print(x_train_Ind59_Normalized)\n",
    "x_test_Ind59_Normalized = x_test_Ind59/255\n",
    "print(x_test_Ind59_Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2345
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1039,
     "status": "ok",
     "timestamp": 1552824911120,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "GZ-WF3Q-fjMl",
    "outputId": "f77af1c8-7ac6-4d1b-c772-5045415aa233"
   },
   "outputs": [],
   "source": [
    "# We use get dummies because when to categorical is used for one hot encoding, it takes from 0 to 4 and 5 to 9 instead of all the way from 0 to 9  even after splitting into 2 halves.\n",
    "# so we go for get dummies and not to categorical\n",
    "import pandas as pd\n",
    "y_train_Ind59_Onehot = pd.get_dummies(y_train_Ind59)\n",
    "print(y_train_Ind59_Onehot)\n",
    "y_test_Ind59_Onehot = pd.get_dummies(y_test_Ind59)\n",
    "print(y_test_Ind59_Onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91169,
     "status": "ok",
     "timestamp": 1552825003270,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "DITyAt3t7Tto",
    "outputId": "fc800db0-e754-4014-ff75-1c06a8910e81"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "model2.fit(x_train_Ind59_Normalized, y_train_Ind59_Onehot, nb_epoch=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        validation_data=(x_test_Ind59_Normalized, y_test_Ind59_Onehot), callbacks=callback_list, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoDozqghCJZ4"
   },
   "source": [
    "## 12. Print the accuracy for classification of digits 5 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3469,
     "status": "ok",
     "timestamp": 1552825077201,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "9fCxgb5s49Cj",
    "outputId": "0e734aa4-0363-44ba-b01f-2862f6e00cba"
   },
   "outputs": [],
   "source": [
    "model2.evaluate(x_train_Ind59, y_train_Ind59_Onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1139,
     "status": "ok",
     "timestamp": 1552825078744,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "LRWizZIpCUKg",
    "outputId": "2946d00a-56e6-4237-d31b-680dc8152d68"
   },
   "outputs": [],
   "source": [
    "model2.evaluate(x_test_Ind59, y_test_Ind59_Onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU-HwvIdH0M-"
   },
   "source": [
    "## Sentiment analysis <br> \n",
    "\n",
    "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
    "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAQDiZHRH0M_"
   },
   "source": [
    "### 13. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1552825326277,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "3eXGIe-SH0NA",
    "outputId": "47b3e0cd-bb76-43e9-eba7-73caef299a90"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount ('/content/drive/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIRyO5ucmaZK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWeWe1eJH0NF"
   },
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('tweets.csv').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPJvTjefH0NI"
   },
   "source": [
    "### 14. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iec5s9gH0NI"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try:\n",
    "        return text.decode('ascii')\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQSmqA-vH0NT"
   },
   "outputs": [],
   "source": [
    "tweets_df['text'] = [preprocess(text) for text in tweets_df.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1420,
     "status": "ok",
     "timestamp": 1552827460370,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "7kX-WoJDH0NV",
    "outputId": "8e049623-7be1-4a5c-c429-9fb3aa3110fb"
   },
   "outputs": [],
   "source": [
    "tweets_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Q-gCNFao1FO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGWB3P2WH0NY"
   },
   "source": [
    "### 15. Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdgA_8N2H0NY"
   },
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[(tweets_df.is_there_an_emotion_directed_at_a_brand_or_product == 'Negative emotion') | (tweets_df.is_there_an_emotion_directed_at_a_brand_or_product =='Positive emotion')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SotCRvkDH0Nf"
   },
   "source": [
    "### 16. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
    "\n",
    "#### Use `vect` as the variable name for initialising CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcbkY4sgH0Ng"
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "tweets_df_dtm = vect.fit_transform(tweets_df['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pxd5fSHH0Nt"
   },
   "source": [
    "### 17. Find number of different words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1199
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1552827463548,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "p1DQ2LdNH0Nu",
    "outputId": "abde248a-cd0a-45db-d370-1b0ed4d38032"
   },
   "outputs": [],
   "source": [
    "dir(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwtgjTBeH0Ny"
   },
   "source": [
    "#### Tip: To see all available functions for an Object use dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 713,
     "status": "ok",
     "timestamp": 1552827465367,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "2n_iCcTNH0N0",
    "outputId": "798692d3-4929-4104-846b-b69ab730fea5"
   },
   "outputs": [],
   "source": [
    "print(len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShA6D8jKH0N5"
   },
   "source": [
    "### 18. Find out how many Positive and Negative emotions are there.\n",
    "\n",
    "Hint: Use value_counts on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1552827466837,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "q7LAl5pzH0N6",
    "outputId": "2d3def24-ee7e-49da-ba74-59fbcb176689"
   },
   "outputs": [],
   "source": [
    "tweets_df['is_there_an_emotion_directed_at_a_brand_or_product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUvgj0FoH0N9"
   },
   "source": [
    "### 19. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n",
    "\n",
    "Hint: use map on that column and give labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YftKwFv7H0N9"
   },
   "outputs": [],
   "source": [
    "tweets_df['Label'] = tweets_df['is_there_an_emotion_directed_at_a_brand_or_product'].map({'Positive emotion':1,'Negative emotion':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### 20. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNkwrGgEH0OA"
   },
   "outputs": [],
   "source": [
    "X = tweets_df['text']\n",
    "y = tweets_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCr_3zD2qt8U"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5nlCuaaH0OD"
   },
   "source": [
    "## 21. **Predicting the sentiment:**\n",
    "\n",
    "\n",
    "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IU0vdrWzu8z5"
   },
   "outputs": [],
   "source": [
    "#tweets_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 962,
     "status": "ok",
     "timestamp": 1552827768630,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "2AbVYssaH0OE",
    "outputId": "8bf0effe-e42e-4b28-dde1-9f0db3f46de1"
   },
   "outputs": [],
   "source": [
    "# use default options for CountVectorizer\n",
    "#vect = CountVectorizer()\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print (accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktXrLhmOH0Of"
   },
   "outputs": [],
   "source": [
    "# import and instantiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1552828624173,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "clv2X0kKH0Ok",
    "outputId": "86dd5bc6-8519-40d7-c0fa-11a1496eb1b6"
   },
   "outputs": [],
   "source": [
    "# train the model using X_train_dtm\n",
    "logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K86LRMfdH0Ou"
   },
   "outputs": [],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2345
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1552828677704,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "CHDOcqiVzMgy",
    "outputId": "2e9e6791-e95f-4aa5-8d86-3ae3addcf8d7"
   },
   "outputs": [],
   "source": [
    "# calculate predicted probabilities for X_test_dtm\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1025,
     "status": "ok",
     "timestamp": 1552828766521,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "7Y_mBrvmzMrP",
    "outputId": "addb9c11-0196-4499-f70f-cf541f2139b2"
   },
   "outputs": [],
   "source": [
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)\n",
    "# calculate accuracy\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw-0B33tH0Ox"
   },
   "source": [
    "## 22. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okCTOs1TH0Oy"
   },
   "outputs": [],
   "source": [
    "def tokenize_predict(vect):\n",
    "    x_train_dtm = vect.fit_transform(x_train)\n",
    "    print('Features: ', x_train_dtm.shape[1])\n",
    "    x_test_dtm = vect.transform(x_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(x_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxZ8jfPEH0O0"
   },
   "source": [
    "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdCyAN_IH0O0"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axepytmgH0O4"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 948,
     "status": "ok",
     "timestamp": 1552829149448,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "HToGkq7vH0O4",
    "outputId": "241d9905-be9f-47dc-9e2c-c35daca3c207"
   },
   "outputs": [],
   "source": [
    "# use default options for CountVectorizer\n",
    "#vect = CountVectorizer()\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vect = CountVectorizer(ngram_range=(1,2),stop_words='english')\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print (accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOIlJRxoH0O7"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1552829192264,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "6fUhff-oH0O8",
    "outputId": "01f08076-8d84-4c16-917d-ef04c2d6109e"
   },
   "outputs": [],
   "source": [
    "# use default options for CountVectorizer\n",
    "#vect = CountVectorizer()\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vect = CountVectorizer(ngram_range=(1,2),stop_words='english',max_features=300)\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print (accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2KZNWVkH0PA"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1552829211661,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "3v9XD082H0PB",
    "outputId": "5390c387-364d-43fa-d9c9-00a30c2a5b7b"
   },
   "outputs": [],
   "source": [
    "# use default options for CountVectorizer\n",
    "#vect = CountVectorizer()\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vect = CountVectorizer(ngram_range=(1,2),stop_words='english',max_features=15000)\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print (accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We3JK_SRH0PO"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1552829242518,
     "user": {
      "displayName": "sairam janakiraman",
      "photoUrl": "",
      "userId": "11677302178638953515"
     },
     "user_tz": -330
    },
    "id": "fUHrfDCyH0PP",
    "outputId": "b52f4918-2c44-4f4c-c0be-f47b591f386b"
   },
   "outputs": [],
   "source": [
    "# use default options for CountVectorizer\n",
    "#vect = CountVectorizer()\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "vect = CountVectorizer(ngram_range=(1,2),min_df=2)\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print (accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H4k_lVZH0PS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R8_Internal_Lab_Questions_17MAR2019_Sairam.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
